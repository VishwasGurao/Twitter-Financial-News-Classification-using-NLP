{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef530c3-ac60-4f3c-96df-45fa058eaafb",
   "metadata": {},
   "source": [
    "# Twitter Financial News Classification  \n",
    "## Final Project Summary\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Objective\n",
    "\n",
    "The objective of this project was to build an end-to-end Natural Language\n",
    "Processing (NLP) pipeline to classify Twitter financial news into multiple\n",
    "financial categories. The project emphasizes practical model selection,\n",
    "efficient feature engineering, and fair comparison of classical machine\n",
    "learning models under real-world resource constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dataset Overview\n",
    "\n",
    "The dataset consists of Twitter posts related to financial news, each labeled\n",
    "into predefined financial categories such as company updates, macroeconomic\n",
    "events, market movements, and corporate actions.\n",
    "\n",
    "Key characteristics of the dataset include:\n",
    "- Multi-class classification problem\n",
    "- Noticeable class imbalance across categories\n",
    "- Short-form, noisy text typical of social media data\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory analysis revealed the following insights:\n",
    "- A small number of categories dominate the dataset\n",
    "- Minority classes contain limited but information-dense samples\n",
    "- Tweet length varies significantly by news category\n",
    "- Class imbalance is a critical challenge affecting model performance\n",
    "\n",
    "EDA guided feature engineering decisions and evaluation strategy selection.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Engineering\n",
    "\n",
    "The following feature engineering techniques were applied:\n",
    "- Text cleaning and normalization\n",
    "- Removal of low-information samples\n",
    "- TF-IDF vectorization using unigrams and bigrams\n",
    "- Feature dimensionality control for computational efficiency\n",
    "\n",
    "This approach ensured consistency and fairness across all model comparisons.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Models Implemented\n",
    "\n",
    "### 5.1 Logistic Regression (Baseline Model)\n",
    "\n",
    "- Served as the primary benchmark model\n",
    "- Performed reliably on majority classes\n",
    "- Provided interpretable and stable results\n",
    "- Used class weighting to mitigate imbalance\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Multinomial Naive Bayes\n",
    "\n",
    "- Computationally efficient and fast to train\n",
    "- Performed adequately on high-frequency classes\n",
    "- Struggled with minority categories due to probabilistic assumptions\n",
    "- Sensitive to class imbalance\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Linear Support Vector Machine (Advanced Classical Model)\n",
    "\n",
    "- Demonstrated the most balanced overall performance\n",
    "- Improved separation between semantically similar categories\n",
    "- Robust to high-dimensional sparse TF-IDF features\n",
    "- Outperformed baseline models in macro-level evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Model Evaluation and Comparison\n",
    "\n",
    "- All models were trained using identical preprocessing and TF-IDF features\n",
    "- Stratified trainâ€“test splits ensured fair evaluation\n",
    "- Confusion matrix analysis highlighted misclassification patterns\n",
    "- Most errors occurred between semantically related financial categories\n",
    "\n",
    "**Final Classical Model Selected:**  \n",
    "**Linear Support Vector Machine (SVM)**\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Deep Learning Exploration (BERT)\n",
    "\n",
    "A BERT-based model was initiated to explore deep learning approaches for text\n",
    "classification. However, due to CPU-only hardware constraints and significantly\n",
    "long training times, the model was not fully trained.\n",
    "\n",
    "The decision to prioritize efficient classical models reflects real-world\n",
    "engineering trade-offs between model complexity, resource availability, and\n",
    "project timelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Key Learnings\n",
    "\n",
    "- Classical machine learning models remain highly effective for text\n",
    "  classification tasks\n",
    "- Feature engineering and data quality often outweigh model complexity\n",
    "- Class imbalance has a strong impact on multi-class performance\n",
    "- Practical constraints should guide model selection decisions\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "This project successfully demonstrates a complete NLP workflow, from data\n",
    "exploration and feature engineering to model training and evaluation. By\n",
    "systematically comparing multiple models, the project identifies Linear SVM\n",
    "as a strong and efficient solution for financial news classification while\n",
    "maintaining practical feasibility.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Project Status\n",
    "\n",
    "**Status:** Completed  \n",
    "**Outcome:** End-to-end, interview-ready NLP classification project\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
