{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6a4f19-c12a-4d61-a0cc-2defbd6d939e",
   "metadata": {},
   "source": [
    "## Advanced Model: BERT for Financial News Classification\n",
    "\n",
    "This notebook implements a BERT-based text classification model to classify\n",
    "Twitter financial news into multiple categories. The objective is to evaluate\n",
    "a deep learning approach and compare its performance with classical models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753fd72e-a483-4305-bd9d-8abb3538990f",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "This step installs and imports the required libraries for training and\n",
    "evaluating a BERT-based text classification model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf272d3-069e-45c5-9688-8ed7d7563d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally and not installed, uncomment the next lines:\n",
    "# !pip install transformers datasets torch accelerate scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "a\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d0639d-5a8c-44ae-8c15-422c8690854c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Tokenizer Initialization (BERT)\n",
    "\n",
    "This step loads the pretrained BERT tokenizer and verifies that the\n",
    "environment is correctly set up for tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd5b57f-9c85-47ca-8912-2a2415d420bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n",
      "Sample tokenized keys: KeysView({'input_ids': [101, 6089, 14831, 13567, 2000, 1996, 2976, 3914, 8874, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Quick sanity check\n",
    "sample_text = \"Markets reacted positively to the Federal Reserve announcement.\"\n",
    "encoded = tokenizer(sample_text, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "print(\"Sample tokenized keys:\", encoded.keys())\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e810a4a-8f55-449a-b127-aa36a16b42bc",
   "metadata": {},
   "source": [
    "## Tokenization of Training and Testing Data\n",
    "\n",
    "This step tokenizes the training and testing text datasets using the\n",
    "pretrained BERT tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f48148e-b9d3-4871-9cea-8a26774c4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 13559\n",
      "Test samples : 3390\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned dataset (if not already loaded)\n",
    "data_path = \"../data/train_clean.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Remove invalid or low-information records\n",
    "df = df.dropna(subset=[\"clean_text\"])\n",
    "df = df[df[\"clean_text\"].str.len() > 10]\n",
    "\n",
    "# Define features and target\n",
    "X = df[\"clean_text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", X_train.shape[0])\n",
    "print(\"Test samples :\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5e8767-901d-4f25-bbf5-2bdb233094b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed successfully.\n",
      "Training samples: 13559\n",
      "Testing samples : 3390\n"
     ]
    }
   ],
   "source": [
    "# Tokenize training data\n",
    "train_encodings = tokenizer(\n",
    "    list(X_train),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Tokenize testing data\n",
    "test_encodings = tokenizer(\n",
    "    list(X_test),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(\"Tokenization completed successfully.\")\n",
    "print(\"Training samples:\", len(train_encodings[\"input_ids\"]))\n",
    "print(\"Testing samples :\", len(test_encodings[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d05eb-4073-473e-bd89-9145bd9edc14",
   "metadata": {},
   "source": [
    "## Trainâ€“Test Split for BERT\n",
    "\n",
    "This step prepares the training and testing datasets required\n",
    "for tokenization and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb34a0a-3cab-4e72-acb8-4ce6092189bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 13559\n",
      "Test samples : 3390\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned dataset\n",
    "data_path = \"../data/train_clean.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Basic cleaning (same as previous notebooks)\n",
    "df = df.dropna(subset=[\"clean_text\"])\n",
    "df = df[df[\"clean_text\"].str.len() > 10]\n",
    "\n",
    "# Define features and target\n",
    "X = df[\"clean_text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", X_train.shape[0])\n",
    "print(\"Test samples :\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0b6c0-0e9b-42cf-b633-06ec0a404187",
   "metadata": {},
   "source": [
    "## Load BERT Tokenizer\n",
    "\n",
    "This step loads the pretrained BERT tokenizer and verifies that the\n",
    "tokenization pipeline is working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c63fc2-c2da-4e92-8d1d-87e9b37d702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n",
      "Tokenized keys: KeysView({'input_ids': [101, 6089, 14831, 13567, 2000, 1996, 2976, 3914, 8874, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Sanity check with a sample sentence\n",
    "sample_text = \"Markets reacted positively to the Federal Reserve announcement.\"\n",
    "encoded = tokenizer(\n",
    "    sample_text,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "print(\"Tokenized keys:\", encoded.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e679dd-2353-4d7b-b36d-207e70359f91",
   "metadata": {},
   "source": [
    "## Tokenize Training and Testing Data\n",
    "\n",
    "This step tokenizes the training and testing text data using the\n",
    "pretrained BERT tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c95f0a-8c6e-4590-98f9-a268bb9cc39c",
   "metadata": {},
   "source": [
    "## Custom PyTorch Dataset for BERT\n",
    "\n",
    "This step creates a PyTorch-compatible dataset required by the\n",
    "HuggingFace Trainer API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5410d22b-5222-4dbe-a503-6e239748ebb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch datasets created successfully.\n",
      "Train dataset size: 13559\n",
      "Test dataset size : 3390\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class FinancialNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels.iloc[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = FinancialNewsDataset(train_encodings, y_train)\n",
    "test_dataset = FinancialNewsDataset(test_encodings, y_test)\n",
    "\n",
    "print(\"PyTorch datasets created successfully.\")\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Test dataset size :\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa6442-e9bd-41b9-9dbe-b88fa74e8da1",
   "metadata": {},
   "source": [
    "## BERT Model Initialization\n",
    "\n",
    "This step initializes a pretrained BERT model for multi-class\n",
    "text classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed05b2bc-4f77-491c-ab62-8870966eb0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 13559\n",
      "Test samples : 3390\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned dataset\n",
    "data_path = \"../data/train_clean.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Basic cleaning\n",
    "df = df.dropna(subset=[\"clean_text\"])\n",
    "df = df[df[\"clean_text\"].str.len() > 10]\n",
    "\n",
    "# Features and target\n",
    "X = df[\"clean_text\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", X_train.shape[0])\n",
    "print(\"Test samples :\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4d0b7-98cf-40a9-b223-d55dea06b857",
   "metadata": {},
   "source": [
    "## Training Configuration for BERT\n",
    "\n",
    "This step defines training arguments optimized for CPU-based training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8861d49-aae8-46c1-9295-e560b57e1fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model initialized successfully with 20 labels.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Number of unique labels\n",
    "num_labels = y_train.nunique()\n",
    "\n",
    "# Initialize pretrained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "print(f\"BERT model initialized successfully with {num_labels} labels.\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_XET_WARNING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a5c4c-00bd-477d-8e6d-acef5bed564b",
   "metadata": {},
   "source": [
    "## Training Configuration for BERT\n",
    "\n",
    "This step defines the training arguments used to fine-tune the BERT model.\n",
    "The configuration is optimized for CPU-based training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46075e46-32c1-4cec-a314-13e95355c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results\",\n",
    "    eval_strategy=\"epoch\",          # <-- FIXED HERE\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./bert_logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f49163-369e-4d9f-a2ec-533f8fb1aa69",
   "metadata": {},
   "source": [
    "## BERT Model Training\n",
    "\n",
    "This step fine-tunes the BERT model on the training dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b6a46b-fcb3-4d3b-89c9-148f99acb476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model initialized successfully with 20 labels.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Number of unique labels\n",
    "num_labels = y_train.nunique()\n",
    "\n",
    "# Initialize pretrained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "print(f\"BERT model initialized successfully with {num_labels} labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ed18c-4f24-48f0-a036-eb226f0016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Training started...\")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40957ba6-6279-4483-81cf-41c554ab5eb1",
   "metadata": {},
   "source": [
    "## Note on BERT Training\n",
    "\n",
    "A BERT-based deep learning model was initiated to explore advanced NLP\n",
    "techniques for financial news classification. However, due to CPU-only\n",
    "hardware constraints and significantly long training time, the model\n",
    "was intentionally stopped before completion.\n",
    "\n",
    "The decision to prioritize efficient classical models such as Linear\n",
    "Support Vector Machine ensured timely project completion while\n",
    "maintaining strong and reliable performance. This reflects a practical\n",
    "engineering trade-off between model complexity, computational resources,\n",
    "and project timelines.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
